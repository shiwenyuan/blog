{"meta":{"title":"shiwenyuan'blog","subtitle":null,"description":"石文远的博客","author":"shiwenyuan","url":"https://shiwenyuan.github.io","root":"/"},"pages":[{"title":"about","date":"2019-08-05T06:46:52.000Z","updated":"2019-08-07T09:18:18.409Z","comments":true,"path":"about/index.html","permalink":"https://shiwenyuan.github.io/about/index.html","excerpt":"","text":"生于忧患，死于安乐。 多思考多沉淀，避免虚度光阴。 每天进步一点点，多年以后再回头，就会发现自己不知不觉走了很远很远。"},{"title":"友情链接","date":"2019-08-08T06:05:45.000Z","updated":"2019-08-08T06:40:02.521Z","comments":true,"path":"links/index.html","permalink":"https://shiwenyuan.github.io/links/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-08-08T06:37:20.000Z","updated":"2019-08-08T06:37:20.464Z","comments":true,"path":"tags/index.html","permalink":"https://shiwenyuan.github.io/tags/index.html","excerpt":"","text":""},{"title":"books","date":"2019-08-08T06:04:56.000Z","updated":"2019-08-08T06:04:56.712Z","comments":true,"path":"books/index.html","permalink":"https://shiwenyuan.github.io/books/index.html","excerpt":"","text":""},{"title":"categories","date":"2019-08-08T06:02:39.000Z","updated":"2019-08-08T06:02:39.904Z","comments":true,"path":"categories/index.html","permalink":"https://shiwenyuan.github.io/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"PHP-多进程之孤儿和僵尸简单讲解","slug":"PHP-多进程之孤儿和僵尸简单讲解","date":"2019-08-07T09:42:13.000Z","updated":"2019-08-07T09:48:24.799Z","comments":true,"path":"posts/cjz2bazgd0005ges6w5y6qvi4.html","link":"","permalink":"https://shiwenyuan.github.io/posts/cjz2bazgd0005ges6w5y6qvi4.html","excerpt":"","text":"PHP 多进程之孤儿 / 僵尸whatinit 进程普通用户运行但有超级权限的进程 孤儿进程父进程在子进程处理完前退出， 子进程就会变成孤儿进程；并由 init 进程进行处理 僵尸进程 「z+」标记 父进程 fork 子进程后， 没有调用 wait 来维护子进程，导致子进程无人管理 why总是不断 fork， 不管维护。系统资源有限 – 就会出现僵尸进程 how相比孤儿进程，僵尸进程危害更大，毕竟孤儿还有 init 妈妈来抚养，僵尸就只能独自游荡！ pcntl_waitpcntl_waitpid 等待或返回 fork 的子进程状态 相同pcntl_wait($status, $option) == pcntl_waitpid(-1, $status, $option)pcntl_waitpid($pid, $status, $option) pid&lt;-1: 等待任意进程组 ID 等于参数 pid 给定值的绝对值的进程-1: 等待任意子进程；与 pcntl_wait 函数行为一致0: 等待任意与调用进程组 ID 相同的子进程 10: 等待进程号等于参数 pid 值的子进程 optionWNOHANG: 如果没有子进程退出立刻返回WUNTRACED: 子进程已经退出并且其状态未报告时返回 孤儿进程12345678910111213141516171819202122232425262728&lt;?php$pid = pcntl_fork();switch($pid) &#123; case 0: // 子进程 echo \"I am son parent's pid=\".posix_getppid().\"\\n\"; for( $i = 1; $i &lt;= 10; $i++ )&#123; sleep( 1 ); // posix_getppid()函数的作用就是获取当前进程的父进程进程ID echo \"parent's pid =\" posix_getppid().PHP_EOL; &#125; break; case -1: // fork error echo \"I am err\\n\"; break; default: // 父进程 echo \"I am parent pid=\".posix_getpid().\"\\n\"; sleep(2); break;&#125;// 第三秒后 由init进程收养 僵尸进程1234567891011121314151617181920&lt;?php$pid = pcntl_fork();switch($pid) &#123; case 0: // 子进程 cli_set_process_title(\"son process php\"); sleep(5); break; case -1: // fork error break; default: // 父进程 cli_set_process_title(\"parent process php\"); sleep(30); break;&#125;// 子进程执行后父进程未取管理导致 解决12345678910111213141516171819202122232425262728&lt;?phpswitch($pid) &#123; case 0: // 子进程 cli_set_process_title(\"son process php\"); sleep(10); break; case -1: // fork error break; default: // 父进程 cli_set_process_title(\"parent process php\"); $iid = pcntl_wait($status); // 等待子进程的状态处理 // 监控子进程状态处理后才能输出 // 只要未触发pcntl_wait，此后的逻辑一直不会处理 echo $iid.\"-----\\n\"; //sleep(60); break;&#125;===// 增加pcntl_wait// pcntl_wait($status); // 只要子进程不退出， 父进程就会阻塞再这个地方 更优解决123456789101112131415161718192021222324252627&lt;?php$pid = pcntl_fork();switch($pid) &#123; case 0: // 子进程 cli_set_process_title(\"son process php\"); sleep(10); break; case -1: // fork error break; default: // 父进程 cli_set_process_title(\"parent process php\"); pcntl_waitpid($pid, $status, WNOHANG); // 监控$pid， 如果没有子进程退出立刻返回 echo \"I am parent\\n\"; //sleep(60); break;&#125;===// pcntl_wait改写pcntl_waitpid// pcntl_waitpid($pid, $status, WNOHANG); // 如果再pcntl_waitpid后续增加sleep， 还是会僵尸进程 更更优解决1信号","categories":[],"tags":[{"name":"PHP","slug":"PHP","permalink":"https://shiwenyuan.github.io/tags/PHP/"},{"name":"Linux","slug":"Linux","permalink":"https://shiwenyuan.github.io/tags/Linux/"}]},{"title":"PHP-面向对象设计的三大特性","slug":"PHP-面向对象设计的三大特性","date":"2019-08-07T09:40:52.000Z","updated":"2019-08-07T09:49:05.758Z","comments":true,"path":"posts/cjz2bazg60002ges6rgog4wld.html","link":"","permalink":"https://shiwenyuan.github.io/posts/cjz2bazg60002ges6rgog4wld.html","excerpt":"","text":"面向对象的三大特性封装，继承，多态 什么是封装？把客观的事物封装成抽象的类，并且类可以把自己的数据和方法只让可信的类或者对象操作，对不可信的类进行信息的隐藏。简单的说就是：封装使对象的设计者与对象的使用者分开，使用者只要知道对象可以做什么就可以了，不需要知道具体是怎么实现的。封装可以有助于提高类和系统的安全性。 什么是继承？继承指的是建立一个新的派生类，从一个或多个先前定义的类中继承数据和函数，可以重新定义或加进新数据和函数，从而建立了类的层次或等级。 什么是多态？多态性指的是： 同一操作作用与不同类的实例，将产生不同的执行结果，即不同类的对象收到相同的消息时，将得到不同的结果。","categories":[],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://shiwenyuan.github.io/tags/设计模式/"}]},{"title":"PHP-面向对象设计的五个基准原则","slug":"PHP-面向对象设计的五个基准原则","date":"2019-08-07T09:34:58.000Z","updated":"2019-08-07T09:39:12.881Z","comments":true,"path":"posts/cjz2bazfq0000ges6syw1vmvd.html","link":"","permalink":"https://shiwenyuan.github.io/posts/cjz2bazfq0000ges6syw1vmvd.html","excerpt":"","text":"前言S.O.L.I.D 是 首个 5 个面向对象设计 (OOD) 准则的首字母缩写这些准则使得开发出易扩展、可维护的软件变得更容易。也使得代码更精简、易于重构。同样也是敏捷开发和自适应软件开发的一部分。 S.O.L.I.D S.O.L.I.D 意思是 S - 单一功能原则O - 开闭原则L - 里氏替换原则I - 接口隔离原则D - 依赖反转原则 单一职责原则一个类有且只能有一个因素使其改变，意思是一个类只应该有单一职责． 开闭原则对象和实体应该对扩展开放，但是对修改关闭． 里氏替换原则如果对每一个类型为 T1 的对象 o1，都有类型为 T2 的对象 o2，使得以 T1 定义的所有程序 P 在所有的对象 o1 都代换成 o2 时，程序 P 的行为没有发生变化，那么类型 T2 是类型 T1 的子类型。 接口隔离原则使用方（client）不应该依赖强制实现不使用的接口，或不应该依赖不使用的方法。 依赖倒置原则实体必须依赖抽象而不是具体的实现．即高等级模块不应该依赖低等级模块，他们都应该依赖抽象．","categories":[],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://shiwenyuan.github.io/tags/设计模式/"}]},{"title":"腾讯面试","slug":"腾讯面试","date":"2019-08-07T09:29:10.000Z","updated":"2019-08-07T09:29:37.267Z","comments":true,"path":"posts/cjz2bazh3000iges6w4sbcxut.html","link":"","permalink":"https://shiwenyuan.github.io/posts/cjz2bazh3000iges6w4sbcxut.html","excerpt":"","text":"本文偶然间在laravel-china上看到过，还不错拿出来和大家分享下，原文请移步 https://learnku.com/articles/28896","categories":[],"tags":[{"name":"面试","slug":"面试","permalink":"https://shiwenyuan.github.io/tags/面试/"}]},{"title":"PHP 面试踩过的坑","slug":"PHP-面试踩过的坑","date":"2019-08-07T09:26:34.000Z","updated":"2019-08-07T09:28:02.419Z","comments":true,"path":"posts/cjz2bazgg0007ges6l6ptxjh7.html","link":"","permalink":"https://shiwenyuan.github.io/posts/cjz2bazgg0007ges6l6ptxjh7.html","excerpt":"","text":"本文偶然间在laravel-china上看到过，还不错拿出来和大家分享下，原文请移步 https://learnku.com/articles/28758","categories":[],"tags":[{"name":"PHP","slug":"PHP","permalink":"https://shiwenyuan.github.io/tags/PHP/"},{"name":"面试","slug":"面试","permalink":"https://shiwenyuan.github.io/tags/面试/"}]},{"title":"Redis “缓存穿透”、“缓存击穿”、“缓存雪崩”","slug":"Redis-“缓存穿透”、“缓存击穿”、“缓存雪崩”","date":"2019-08-07T09:16:18.000Z","updated":"2019-08-07T09:16:56.260Z","comments":true,"path":"posts/cjz2bazgk0009ges67lln9bt9.html","link":"","permalink":"https://shiwenyuan.github.io/posts/cjz2bazgk0009ges67lln9bt9.html","excerpt":"","text":"前几天去面试了，吃了点亏。面试官问我，你知道 “缓存穿透”、“缓存击穿”、“缓存雪崩” 吗？我当时确实不知道这些高大上的名词是什么意思，就是我没听过，不知道是啥意思，面试官说，你这方面好薄弱啊～～～面试结束之后我百度了一下，我去，看起来高大上的名字，其实是很常见的场景，相信很多人都会处理，缓存穿透 ： DB 承受了没有必要的查询流量，意思就是查到空值的时候没有做缓存处理，再次查询的时候继续读库了缓存击穿：热点 Key，大量并发读请求引起的小雪崩， 就是缓存在某个时间点过期的时候，恰好在这个时间点对这个 Key 有大量的并发请求过来，这些请求发现缓存过期一般都会从后端 DB 加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端 DB 压垮缓存雪崩：缓存设置同一过期时间，引发的大量的读取数据库操作 分享一个讲解的链接：https://www.jianshu.com/p/fef1c22d63cb 专业术语还是得多了解一下，不然即使自己能解决的问题，别人用专业术语问的也不知道是个什么东西","categories":[],"tags":[{"name":"面试","slug":"面试","permalink":"https://shiwenyuan.github.io/tags/面试/"},{"name":"redis","slug":"redis","permalink":"https://shiwenyuan.github.io/tags/redis/"},{"name":"db","slug":"db","permalink":"https://shiwenyuan.github.io/tags/db/"}]},{"title":"单例模式（Singleton）","slug":"单例模式（Singleton）","date":"2019-08-07T08:47:39.000Z","updated":"2019-08-07T08:53:29.145Z","comments":true,"path":"posts/cjz2bazh1000hges6dh2glor5.html","link":"","permalink":"https://shiwenyuan.github.io/posts/cjz2bazh1000hges6dh2glor5.html","excerpt":"","text":"目的在应用程序调用的时候，只能获得一个对象实例。 例子数据库连接 UML 类图 特点 三私一公 私有静态成员变量私有构造函数私有克隆函数公共静态方法 实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546cat Singleton.php&lt;?phpnamespace DesignPatterns\\Creational\\Singleton;final class Singleton&#123; /** * @var Singleton */ private static $instance; /** * 通过懒加载获得实例（在第一次使用的时候创建） */ public static function getInstance(): Singleton &#123; if (null === static::$instance) &#123; static::$instance = new static(); &#125; return static::$instance; &#125; /** * 不允许从外部调用以防止创建多个实例 * 要使用单例，必须通过 Singleton::getInstance() 方法获取实例 */ private function __construct() &#123; &#125; /** * 防止实例被克隆（这会创建实例的副本） */ private function __clone() &#123; &#125; /** * 防止反序列化（这将创建它的副本） */ private function __wakeup() &#123; &#125;&#125;","categories":[],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://shiwenyuan.github.io/tags/设计模式/"}]},{"title":"MySQL 索引原理以及优化","slug":"MySQL-索引原理以及优化","date":"2019-08-07T08:22:24.000Z","updated":"2019-08-07T09:15:39.042Z","comments":true,"path":"posts/cjz2bazj2001cges6d5ovabh7.html","link":"","permalink":"https://shiwenyuan.github.io/posts/cjz2bazj2001cges6d5ovabh7.html","excerpt":"","text":"前言 本文是美团一位大佬写的，还不错拿出来和大家分享下，原文请移步 https://tech.meituan.com/2014/06/30/mysql-index.html 背景 MySQL 凭借着出色的性能、低廉的成本、丰富的资源，已经成为绝大多数互联网公司的首选关系型数据库。虽然性能出色，但所谓 “好马配好鞍”，如何能够更好的使用它，已经成为开发工程师的必修课，我们经常会从职位描述上看到诸如 “精通 MySQL”、“SQL 语句优化”、“了解数据库原理” 等要求。我们知道一般的应用系统，读写比例在 10:1 左右，而且插入操作和一般的更新操作很少出现性能问题，遇到最多的，也是最容易出问题的，还是一些复杂的查询操作，所以查询语句的优化显然是重中之重。 本人从 13 年 7 月份起，一直在美团核心业务系统部做慢查询的优化工作，共计十余个系统，累计解决和积累了上百个慢查询案例。随着业务的复杂性提升，遇到的问题千奇百怪，五花八门，匪夷所思。本文旨在以开发工程师的角度来解释数据库索引的原理和如何优化慢查询。 一个慢查询引发的思考12345678910select count(*) from task where status=2 and operator_id=20839 and operate_time&gt;1371169729 and operate_time&lt;1371174603 and type=2; 系统使用者反应有一个功能越来越慢，于是工程师找到了上面的SQL。 并且兴致冲冲的找到了我，“这个SQL需要优化，给我把每个字段都加上索引”。 我很惊讶，问道：“为什么需要每个字段都加上索引？” “把查询的字段都加上索引会更快”，工程师信心满满。 “这种情况完全可以建一个联合索引，因为是最左前缀匹配，所以operate_time需要放到最后，而且还需要把其他相关的查询都拿来，需要做一个综合评估。” “联合索引？最左前缀匹配？综合评估？”工程师不禁陷入了沉思。 多数情况下，我们知道索引能够提高查询效率，但应该如何建立索引？索引的顺序如何？许多人却只知道大概。其实理解这些概念并不难，而且索引的原理远没有想象的那么复杂。 MySQL索引原理索引目的索引的目的在于提高查询效率，可以类比字典，如果要查“mysql”这个单词，我们肯定需要定位到m字母，然后从下往下找到y字母，再找到剩下的sql。如果没有索引，那么你可能需要把所有单词看一遍才能找到你想要的，如果我想找到m开头的单词呢？或者ze开头的单词呢？是不是觉得如果没有索引，这个事情根本无法完成？ 索引原理除了词典，生活中随处可见索引的例子，如火车站的车次表、图书的目录等。它们的原理都是一样的，通过不断的缩小想要获得数据的范围来筛选出最终想要的结果，同时把随机的事件变成顺序的事件，也就是我们总是通过同一种查找方式来锁定数据。 数据库也是一样，但显然要复杂许多，因为不仅面临着等值查询，还有范围查询(&gt;、&lt;、between、in)、模糊查询(like)、并集查询(or)等等。数据库应该选择怎么样的方式来应对所有的问题呢？我们回想字典的例子，能不能把数据分成段，然后分段查询呢？最简单的如果1000条数据，1到100分成第一段，101到200分成第二段，201到300分成第三段……这样查第250条数据，只要找第三段就可以了，一下子去除了90%的无效数据。但如果是1千万的记录呢，分成几段比较好？稍有算法基础的同学会想到搜索树，其平均复杂度是lgN，具有不错的查询性能。但这里我们忽略了一个关键的问题，复杂度模型是基于每次相同的操作成本来考虑的，数据库实现比较复杂，数据保存在磁盘上，而为了提高性能，每次又可以把部分数据读入内存来计算，因为我们知道访问磁盘的成本大概是访问内存的十万倍左右，所以简单的搜索树难以满足复杂的应用场景。 磁盘IO与预读前面提到了访问磁盘，那么这里先简单介绍一下磁盘IO和预读，磁盘读取数据靠的是机械运动，每次读取数据花费的时间可以分为寻道时间、旋转延迟、传输时间三个部分，寻道时间指的是磁臂移动到指定磁道所需要的时间，主流磁盘一般在5ms以下；旋转延迟就是我们经常听说的磁盘转速，比如一个磁盘7200转，表示每分钟能转7200次，也就是说1秒钟能转120次，旋转延迟就是1/120/2 = 4.17ms；传输时间指的是从磁盘读出或将数据写入磁盘的时间，一般在零点几毫秒，相对于前两个时间可以忽略不计。那么访问一次磁盘的时间，即一次磁盘IO的时间约等于5+4.17 = 9ms左右，听起来还挺不错的，但要知道一台500 -MIPS的机器每秒可以执行5亿条指令，因为指令依靠的是电的性质，换句话说执行一次IO的时间可以执行40万条指令，数据库动辄十万百万乃至千万级数据，每次9毫秒的时间，显然是个灾难。下图是计算机硬件延迟的对比图，供大家参考： 考虑到磁盘IO是非常高昂的操作，计算机操作系统做了一些优化，当一次IO时，不光把当前磁盘地址的数据，而是把相邻的数据也都读取到内存缓冲区内，因为局部预读性原理告诉我们，当计算机访问一个地址的数据的时候，与其相邻的数据也会很快被访问到。每一次IO读取的数据我们称之为一页(page)。具体一页有多大数据跟操作系统有关，一般为4k或8k，也就是我们读取一页内的数据时候，实际上才发生了一次IO，这个理论对于索引的数据结构设计非常有帮助。 索引的数据结构前面讲了生活中索引的例子，索引的基本原理，数据库的复杂性，又讲了操作系统的相关知识，目的就是让大家了解，任何一种数据结构都不是凭空产生的，一定会有它的背景和使用场景，我们现在总结一下，我们需要这种数据结构能够做些什么，其实很简单，那就是：每次查找数据时把磁盘IO次数控制在一个很小的数量级，最好是常数数量级。那么我们就想到如果一个高度可控的多路搜索树是否能满足需求呢？就这样，b+树应运而生。 详解b+树 如上图，是一颗b+树，关于b+树的定义可以参见B+树，这里只说一些重点，浅蓝色的块我们称之为一个磁盘块，可以看到每个磁盘块包含几个数据项（深蓝色所示）和指针（黄色所示），如磁盘块1包含数据项17和35，包含指针P1、P2、P3，P1表示小于17的磁盘块，P2表示在17和35之间的磁盘块，P3表示大于35的磁盘块。真实的数据存在于叶子节点即3、5、9、10、13、15、28、29、36、60、75、79、90、99。非叶子节点只不存储真实的数据，只存储指引搜索方向的数据项，如17、35并不真实存在于数据表中。 b+树的查找过程如图所示，如果要查找数据项29，那么首先会把磁盘块1由磁盘加载到内存，此时发生一次IO，在内存中用二分查找确定29在17和35之间，锁定磁盘块1的P2指针，内存时间因为非常短（相比磁盘的IO）可以忽略不计，通过磁盘块1的P2指针的磁盘地址把磁盘块3由磁盘加载到内存，发生第二次IO，29在26和30之间，锁定磁盘块3的P2指针，通过指针加载磁盘块8到内存，发生第三次IO，同时内存中做二分查找找到29，结束查询，总计三次IO。真实的情况是，3层的b+树可以表示上百万的数据，如果上百万的数据查找只需要三次IO，性能提高将是巨大的，如果没有索引，每个数据项都要发生一次IO，那么总共需要百万次的IO，显然成本非常非常高。 b+树性质1.通过上面的分析，我们知道IO次数取决于b+数的高度h，假设当前数据表的数据为N，每个磁盘块的数据项的数量是m，则有h=㏒(m+1)N，当数据量N一定的情况下，m越大，h越小；而m = 磁盘块的大小 / 数据项的大小，磁盘块的大小也就是一个数据页的大小，是固定的，如果数据项占的空间越小，数据项的数量越多，树的高度越低。这就是为什么每个数据项，即索引字段要尽量的小，比如int占4字节，要比bigint8字节少一半。这也是为什么b+树要求把真实的数据放到叶子节点而不是内层节点，一旦放到内层节点，磁盘块的数据项会大幅度下降，导致树增高。当数据项等于1时将会退化成线性表。 2.当b+树的数据项是复合的数据结构，比如(name,age,sex)的时候，b+数是按照从左到右的顺序来建立搜索树的，比如当(张三,20,F)这样的数据来检索的时候，b+树会优先比较name来确定下一步的所搜方向，如果name相同再依次比较age和sex，最后得到检索的数据；但当(20,F)这样的没有name的数据来的时候，b+树就不知道下一步该查哪个节点，因为建立搜索树的时候name就是第一个比较因子，必须要先根据name来搜索才能知道下一步去哪里查询。比如当(张三,F)这样的数据来检索时，b+树可以用name来指定搜索方向，但下一个字段age的缺失，所以只能把名字等于张三的数据都找到，然后再匹配性别是F的数据了， 这个是非常重要的性质，即索引的最左匹配特性。 慢查询优化关于MySQL索引原理是比较枯燥的东西，大家只需要有一个感性的认识，并不需要理解得非常透彻和深入。我们回头来看看一开始我们说的慢查询，了解完索引原理之后，大家是不是有什么想法呢？先总结一下索引的几大基本原则： 建索引的几大原则1.最左前缀匹配原则，非常重要的原则，mysql会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配，比如a = 1 and b = 2 and c &gt; 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。 2.=和in可以乱序，比如a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式。 3.尽量选择区分度高的列作为索引，区分度的公式是count(distinct col)/count(*)，表示字段不重复的比例，比例越大我们扫描的记录数越少，唯一键的区分度是1，而一些状态、性别字段可能在大数据面前区分度就是0，那可能有人会问，这个比例有什么经验值吗？使用场景不同，这个值也很难确定，一般需要join的字段我们都要求是0.1以上，即平均1条扫描10条记录。 4.索引列不能参与计算，保持列“干净”，比如from_unixtime(create_time) = ’2014-05-29’就不能使用到索引，原因很简单，b+树中存的都是数据表中的字段值，但进行检索时，需要把所有元素都应用函数才能比较，显然成本太大。所以语句应该写成create_time = unix_timestamp(’2014-05-29’)。 5.尽量的扩展索引，不要新建索引。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可。 回到开始的慢查询根据最左匹配原则，最开始的sql语句的索引应该是status、operator_id、type、operate_time的联合索引；其中status、operator_id、type的顺序可以颠倒，所以我才会说，把这个表的所有相关查询都找到，会综合分析；比如还有如下查询： 1select * from task where status = 0 and type = 12 limit 10; 1select count(*) from task where status = 0 ; 那么索引建立成(status,type,operator_id,operate_time)就是非常正确的，因为可以覆盖到所有情况。这个就是利用了索引的最左匹配的原则 查询优化神器 - explain命令关于explain命令相信大家并不陌生，具体用法和字段含义可以参考官网explain-output，这里需要强调rows是核心指标，绝大部分rows小的语句执行一定很快（有例外，下面会讲到）。所以优化语句基本上都是在优化rows。 慢查询优化基本步骤0.先运行看看是否真的很慢，注意设置SQL_NO_CACHE 1.where条件单表查，锁定最小返回记录表。这句话的意思是把查询语句的where都应用到表中返回的记录数最小的表开始查起，单表每个字段分别查询，看哪个字段的区分度最高 2.explain查看执行计划，是否与1预期一致（从锁定记录较少的表开始查询） 3.order by limit 形式的sql语句让排序的表优先查 4.了解业务方使用场景 5.加索引时参照建索引的几大原则 6.观察结果，不符合预期继续从0分析 几个慢查询案例下面几个例子详细解释了如何分析和优化慢查询。 复杂语句写法很多情况下，我们写SQL只是为了实现功能，这只是第一步，不同的语句书写方式对于效率往往有本质的差别，这要求我们对mysql的执行计划和索引原则有非常清楚的认识，请看下面的语句： 12345678910111213141516171819202122232425262728select distinct cert.emp_id from cm_log cl inner join ( select emp.id as emp_id, emp_cert.id as cert_id from employee emp left join emp_certificate emp_cert on emp.id = emp_cert.emp_id where emp.is_deleted=0 ) cert on ( cl.ref_table='Employee' and cl.ref_oid= cert.emp_id ) or ( cl.ref_table='EmpCertificate' and cl.ref_oid= cert.cert_id ) where cl.last_upd_date &gt;='2013-11-07 15:03:00' and cl.last_upd_date&lt;='2013-11-08 16:00:00'; 0.先运行一下，53条记录 1.87秒，又没有用聚合语句，比较慢 153 rows in set (1.87 sec) 1.explain 12345678+----+-------------+------------+-------+---------------------------------+-----------------------+---------+-------------------+-------+--------------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+-------+---------------------------------+-----------------------+---------+-------------------+-------+--------------------------------+| 1 | PRIMARY | cl | range | cm_log_cls_id,idx_last_upd_date | idx_last_upd_date | 8 | NULL | 379 | Using where; Using temporary || 1 | PRIMARY | &lt;derived2&gt; | ALL | NULL | NULL | NULL | NULL | 63727 | Using where; Using join buffer || 2 | DERIVED | emp | ALL | NULL | NULL | NULL | NULL | 13317 | Using where || 2 | DERIVED | emp_cert | ref | emp_certificate_empid | emp_certificate_empid | 4 | meituanorg.emp.id | 1 | Using index |+----+-------------+------------+-------+---------------------------------+-----------------------+---------+-------------------+-------+--------------------------------+ 简述一下执行计划，首先mysql根据idx_last_upd_date索引扫描cm_log表获得379条记录；然后查表扫描了63727条记录，分为两部分，derived表示构造表，也就是不存在的表，可以简单理解成是一个语句形成的结果集，后面的数字表示语句的ID。derived2表示的是ID = 2的查询构造了虚拟表，并且返回了63727条记录。我们再来看看ID = 2的语句究竟做了写什么返回了这么大量的数据，首先全表扫描employee表13317条记录，然后根据索引emp_certificate_empid关联emp_certificate表，rows = 1表示，每个关联都只锁定了一条记录，效率比较高。获得后，再和cm_log的379条记录根据规则关联。从执行过程上可以看出返回了太多的数据，返回的数据绝大部分cm_log都用不到，因为cm_log只锁定了379条记录。 如何优化呢？可以看到我们在运行完后还是要和cm_log做join,那么我们能不能之前和cm_log做join呢？仔细分析语句不难发现，其基本思想是如果cm_log的ref_table是EmpCertificate就关联emp_certificate表，如果ref_table是Employee就关联employee表，我们完全可以拆成两部分，并用union连接起来，注意这里用union，而不用union all是因为原语句有“distinct”来得到唯一的记录，而union恰好具备了这种功能。如果原语句中没有distinct不需要去重，我们就可以直接使用union all了，因为使用union需要去重的动作，会影响SQL性能。 优化过的语句如下： 12345678910111213141516171819202122232425262728select emp.id from cm_log cl inner join employee emp on cl.ref_table = 'Employee' and cl.ref_oid = emp.id where cl.last_upd_date &gt;='2013-11-07 15:03:00' and cl.last_upd_date&lt;='2013-11-08 16:00:00' and emp.is_deleted = 0 unionselect emp.id from cm_log cl inner join emp_certificate ec on cl.ref_table = 'EmpCertificate' and cl.ref_oid = ec.id inner join employee emp on emp.id = ec.emp_id where cl.last_upd_date &gt;='2013-11-07 15:03:00' and cl.last_upd_date&lt;='2013-11-08 16:00:00' and emp.is_deleted = 0 4.不需要了解业务场景，只需要改造的语句和改造之前的语句保持结果一致 5.现有索引可以满足，不需要建索引 6.用改造后的语句实验一下，只需要10ms 降低了近200倍！ 1234567891011+----+--------------+------------+--------+---------------------------------+-------------------+---------+-----------------------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+--------------+------------+--------+---------------------------------+-------------------+---------+-----------------------+------+-------------+| 1 | PRIMARY | cl | range | cm_log_cls_id,idx_last_upd_date | idx_last_upd_date | 8 | NULL | 379 | Using where || 1 | PRIMARY | emp | eq_ref | PRIMARY | PRIMARY | 4 | meituanorg.cl.ref_oid | 1 | Using where || 2 | UNION | cl | range | cm_log_cls_id,idx_last_upd_date | idx_last_upd_date | 8 | NULL | 379 | Using where || 2 | UNION | ec | eq_ref | PRIMARY,emp_certificate_empid | PRIMARY | 4 | meituanorg.cl.ref_oid | 1 | || 2 | UNION | emp | eq_ref | PRIMARY | PRIMARY | 4 | meituanorg.ec.emp_id | 1 | Using where || NULL | UNION RESULT | &lt;union1,2&gt; | ALL | NULL | NULL | NULL | NULL | NULL | |+----+--------------+------------+--------+---------------------------------+-------------------+---------+-----------------------+------+-------------+53 rows in set (0.01 sec) 明确应用场景举这个例子的目的在于颠覆我们对列的区分度的认知，一般上我们认为区分度越高的列，越容易锁定更少的记录，但在一些特殊的情况下，这种理论是有局限性的。 1234567891011select * from stage_poi sp where sp.accurate_result=1 and ( sp.sync_status=0 or sp.sync_status=2 or sp.sync_status=4 ); 0.先看看运行多长时间,951条数据6.22秒，真的很慢。 1951 rows in set (6.22 sec) 1.先explain，rows达到了361万，type = ALL表明是全表扫描。 12345+----+-------------+-------+------+---------------+------+---------+------+---------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+------+---------------+------+---------+------+---------+-------------+| 1 | SIMPLE | sp | ALL | NULL | NULL | NULL | NULL | 3613155 | Using where |+----+-------------+-------+------+---------------+------+---------+------+---------+-------------+ 2.所有字段都应用查询返回记录数，因为是单表查询 0已经做过了951条。 3.让explain的rows 尽量逼近951。 看一下accurate_result = 1的记录数： 12345678select count(*),accurate_result from stage_poi group by accurate_result;+----------+-----------------+| count(*) | accurate_result |+----------+-----------------+| 1023 | -1 || 2114655 | 0 || 972815 | 1 |+----------+-----------------+ 我们看到accurate_result这个字段的区分度非常低，整个表只有-1,0,1三个值，加上索引也无法锁定特别少量的数据。 再看一下sync_status字段的情况： 1234567select count(*),sync_status from stage_poi group by sync_status;+----------+-------------+| count(*) | sync_status |+----------+-------------+| 3080 | 0 || 3085413 | 3 |+----------+-------------+ 同样的区分度也很低，根据理论，也不适合建立索引。 问题分析到这，好像得出了这个表无法优化的结论，两个列的区分度都很低，即便加上索引也只能适应这种情况，很难做普遍性的优化，比如当sync_status 0、3分布的很平均，那么锁定记录也是百万级别的。 4.找业务方去沟通，看看使用场景。业务方是这么来使用这个SQL语句的，每隔五分钟会扫描符合条件的数据，处理完成后把sync_status这个字段变成1,五分钟符合条件的记录数并不会太多，1000个左右。了解了业务方的使用场景后，优化这个SQL就变得简单了，因为业务方保证了数据的不平衡，如果加上索引可以过滤掉绝大部分不需要的数据。 5.根据建立索引规则，使用如下语句建立索引 1alter table stage_poi add index idx_acc_status(accurate_result,sync_status); 6.观察预期结果,发现只需要200ms，快了30多倍。 1952 rows in set (0.20 sec) 我们再来回顾一下分析问题的过程，单表查询相对来说比较好优化，大部分时候只需要把where条件里面的字段依照规则加上索引就好，如果只是这种“无脑”优化的话，显然一些区分度非常低的列，不应该加索引的列也会被加上索引，这样会对插入、更新性能造成严重的影响，同时也有可能影响其它的查询语句。所以我们第4步调差SQL的使用场景非常关键，我们只有知道这个业务场景，才能更好地辅助我们更好的分析和优化查询语句。 无法优化的语句12345678910111213141516171819202122232425262728293031323334353637select c.id, c.name, c.position, c.sex, c.phone, c.office_phone, c.feature_info, c.birthday, c.creator_id, c.is_keyperson, c.giveup_reason, c.status, c.data_source, from_unixtime(c.created_time) as created_time, from_unixtime(c.last_modified) as last_modified, c.last_modified_user_id from contact c inner join contact_branch cb on c.id = cb.contact_id inner join branch_user bu on cb.branch_id = bu.branch_id and bu.status in ( 1, 2) inner join org_emp_info oei on oei.data_id = bu.user_id and oei.node_left &gt;= 2875 and oei.node_right &lt;= 10802 and oei.org_category = - 1 order by c.created_time desc limit 0 , 10; 还是几个步骤。 0.先看语句运行多长时间，10条记录用了13秒，已经不可忍受。 110 rows in set (13.06 sec) 1.explain 12345678+----+-------------+-------+--------+-------------------------------------+-------------------------+---------+--------------------------+------+----------------------------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+--------+-------------------------------------+-------------------------+---------+--------------------------+------+----------------------------------------------+| 1 | SIMPLE | oei | ref | idx_category_left_right,idx_data_id | idx_category_left_right | 5 | const | 8849 | Using where; Using temporary; Using filesort || 1 | SIMPLE | bu | ref | PRIMARY,idx_userid_status | idx_userid_status | 4 | meituancrm.oei.data_id | 76 | Using where; Using index || 1 | SIMPLE | cb | ref | idx_branch_id,idx_contact_branch_id | idx_branch_id | 4 | meituancrm.bu.branch_id | 1 | || 1 | SIMPLE | c | eq_ref | PRIMARY | PRIMARY | 108 | meituancrm.cb.contact_id | 1 | |+----+-------------+-------+--------+-------------------------------------+-------------------------+---------+--------------------------+------+----------------------------------------------+ 从执行计划上看，mysql先查org_emp_info表扫描8849记录，再用索引idx_userid_status关联branch_user表，再用索引idx_branch_id关联contact_branch表，最后主键关联contact表。 rows返回的都非常少，看不到有什么异常情况。我们在看一下语句，发现后面有order by + limit组合，会不会是排序量太大搞的？于是我们简化SQL，去掉后面的order by 和 limit，看看到底用了多少记录来排序。 12345678910111213141516171819202122232425select count(*)from contact c inner join contact_branch cb on c.id = cb.contact_id inner join branch_user bu on cb.branch_id = bu.branch_id and bu.status in ( 1, 2) inner join org_emp_info oei on oei.data_id = bu.user_id and oei.node_left &gt;= 2875 and oei.node_right &lt;= 10802 and oei.org_category = - 1 +----------+| count(*) |+----------+| 778878 |+----------+1 row in set (5.19 sec) 发现排序之前居然锁定了778878条记录，如果针对70万的结果集排序，将是灾难性的，怪不得这么慢，那我们能不能换个思路，先根据contact的created_time排序，再来join会不会比较快呢？ 于是改造成下面的语句，也可以用straight_join来优化： 12345678910111213141516171819202122232425262728293031323334353637383940414243select c.id, c.name, c.position, c.sex, c.phone, c.office_phone, c.feature_info, c.birthday, c.creator_id, c.is_keyperson, c.giveup_reason, c.status, c.data_source, from_unixtime(c.created_time) as created_time, from_unixtime(c.last_modified) as last_modified, c.last_modified_user_id from contact c where exists ( select 1 from contact_branch cb inner join branch_user bu on cb.branch_id = bu.branch_id and bu.status in ( 1, 2) inner join org_emp_info oei on oei.data_id = bu.user_id and oei.node_left &gt;= 2875 and oei.node_right &lt;= 10802 and oei.org_category = - 1 where c.id = cb.contact_id ) order by c.created_time desc limit 0 , 10; 验证一下效果 预计在1ms内，提升了13000多倍！ 110 rows in set (0.00 sec) 本以为至此大工告成，但我们在前面的分析中漏了一个细节，先排序再join和先join再排序理论上开销是一样的，为何提升这么多是因为有一个limit！大致执行过程是：mysql先按索引排序得到前10条记录，然后再去join过滤，当发现不够10条的时候，再次去10条，再次join，这显然在内层join过滤的数据非常多的时候，将是灾难的，极端情况，内层一条数据都找不到，mysql还傻乎乎的每次取10条，几乎遍历了这个数据表！ 用不同参数的SQL试验下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344select sql_no_cache c.id, c.name, c.position, c.sex, c.phone, c.office_phone, c.feature_info, c.birthday, c.creator_id, c.is_keyperson, c.giveup_reason, c.status, c.data_source, from_unixtime(c.created_time) as created_time, from_unixtime(c.last_modified) as last_modified, c.last_modified_user_id from contact c where exists ( select 1 from contact_branch cb inner join branch_user bu on cb.branch_id = bu.branch_id and bu.status in ( 1, 2) inner join org_emp_info oei on oei.data_id = bu.user_id and oei.node_left &gt;= 2875 and oei.node_right &lt;= 2875 and oei.org_category = - 1 where c.id = cb.contact_id ) order by c.created_time desc limit 0 , 10;Empty set (2 min 18.99 sec) 2 min 18.99 sec！比之前的情况还糟糕很多。由于mysql的nested loop机制，遇到这种情况，基本是无法优化的。这条语句最终也只能交给应用系统去优化自己的逻辑了。 通过这个例子我们可以看到，并不是所有语句都能优化，而往往我们优化时，由于SQL用例回归时落掉一些极端情况，会造成比原来还严重的后果。所以，第一：不要指望所有语句都能通过SQL优化，第二：不要过于自信，只针对具体case来优化，而忽略了更复杂的情况。 慢查询的案例就分析到这儿，以上只是一些比较典型的案例。我们在优化过程中遇到过超过1000行，涉及到16个表join的“垃圾SQL”，也遇到过线上线下数据库差异导致应用直接被慢查询拖死，也遇到过varchar等值比较没有写单引号，还遇到过笛卡尔积查询直接把从库搞死。再多的案例其实也只是一些经验的积累，如果我们熟悉查询优化器、索引的内部原理，那么分析这些案例就变得特别简单了。 写在后面的话本文以一个慢查询案例引入了MySQL索引原理、优化慢查询的一些方法论;并针对遇到的典型案例做了详细的分析。其实做了这么长时间的语句优化后才发现，任何数据库层面的优化都抵不上应用系统的优化，同样是MySQL，可以用来支撑Google/FaceBook/Taobao应用，但可能连你的个人网站都撑不住。套用最近比较流行的话：“查询容易，优化不易，且写且珍惜！” 参考文献：1.《高性能MySQL》 2.《数据结构与算法分析》","categories":[],"tags":[{"name":"db","slug":"db","permalink":"https://shiwenyuan.github.io/tags/db/"},{"name":"Mysql","slug":"Mysql","permalink":"https://shiwenyuan.github.io/tags/Mysql/"},{"name":"优化","slug":"优化","permalink":"https://shiwenyuan.github.io/tags/优化/"}]},{"title":"科学上网","slug":"mac科学上网","date":"2019-08-07T02:25:13.000Z","updated":"2019-08-07T07:51:39.598Z","comments":true,"path":"posts/cjz2bazgv000dges6zpo6ubm1.html","link":"","permalink":"https://shiwenyuan.github.io/posts/cjz2bazgv000dges6zpo6ubm1.html","excerpt":"","text":"https://glados.network/console邮箱注册、初始免费用30t，亲测有效 这个网站是我偶然间听别人说的，就抱着试一试的态度去注册了一下，结果非常令人开心，要比我之前用到那个网站快很多，本着好东西就要分享出来的态度分享给大家 当前的网站还在内测阶段所以注册时候必须要一个邀请码:DEC8I-ANGH8-XBZ34-NL33I","categories":[],"tags":[{"name":"系统工具","slug":"系统工具","permalink":"https://shiwenyuan.github.io/tags/系统工具/"}]},{"title":"nginx配置https","slug":"nginx配置https","date":"2019-08-06T03:38:32.000Z","updated":"2019-08-06T03:41:47.283Z","comments":true,"path":"posts/cjz2bazhz0015ges6m9mf4gvd.html","link":"","permalink":"https://shiwenyuan.github.io/posts/cjz2bazhz0015ges6m9mf4gvd.html","excerpt":"","text":"网站https网站https化已是大势所趋，个人blog也都可以把https玩儿起来！ Let’s Encrypt123这个免费、自动化、开放的证书签发服务。它由 ISRG（Internet Security Research Group，互联网安全研究小组）提供服务，而 ISRG 是来自于美国加利福尼亚州的一个公益组织。Let&apos;s Encrypt 得到了 Mozilla、Cisco、Akamai、Electronic Frontier Foundation 和 Chrome 等众多公司和机构的支持，发展十分迅猛。申请 Let&apos;s Encrypt 证书不但免费，还非常简单，虽然每次只有 90 天的有效期，但可以通过脚本定期更新，配好之后一劳永逸。经过一段时间的观望，我也正式启用 Let&apos;s Encrypt 证书了，本文记录本站申请过程和遇到的问题。我没有使用 Let&apos;s Encrypt 官网提供的工具来申请证书，而是用了 [acme.sh](http://https://github.com/Neilpang/acme.sh &quot;acme.sh&quot;) 这个更为小巧的开源工具。以下内容基本按照 acme的说明文档写的，省略了一些我不需要的步骤。 配置验证服务1传统 CA 的验证方式一般是往 admin@youremail.com 发验证邮件，而 Let&apos;s Encrypt 是在你的服务器上生成一个随机验证文件，再通过创建 CSR 时指定的域名访问，如果可以访问则表明你对这个域名有控制权。 配置前提11. nginx安装了https模块 通过web访问check域名权限步骤1（建立目录或者nginx访问规则）CA认证12345location ^~ /.well-known/acme-challenge/ &#123; # 注：这里的$challenges_dir请替换成你自己的真实目录，如：/home/work/www/challenges/ alias $challenges_dir; try_files $uri =404;&#125; or 12在项目根目录添加.well-known/acme-challengeLet&apos;s Encrypt 用来校验网站权限 步骤二 生成证书1./acme.sh --issue -d diancan.xiaochengxu.phpblog.com.cn --webroot /home/www/xiaochengxu/diancan 步骤三 cp证书到指定位置1234acme.sh --installcert -d www.your-app.com \\ --keypath /usr/local/nginx/ssl/diancan.xiaochengxu.phpblog.com.cn.key \\ --fullchainpath /usr/local/nginx/ssl/diancan.xiaochengxu.phpblog.com.cn.key.pem \\ --reloadcmd &quot; /usr/local/nginx/sbin/nginx -s reload&quot; 步骤四 配置nginx1234567891011121314151617181920212223242526server &#123; listen 80; server_name diancan.xiaochengxu.phpblog.com.cn; location / &#123; rewrite ^/(.*)$ https://diancan.xiaochengxu.phpblog.com.cn; &#125;&#125;server &#123; listen 443 ssl; server_name diancan.xiaochengxu.phpblog.com.cn; include /usr/local/nginx/ssl/ssl_params; ssl_certificate /usr/local/nginx/ssl/diancan.xiaochengxu.phpblog.com.cn/diancan.xiaochengxu.phpblog.com.cn.cer; ssl_certificate_key /usr/local/nginx/ssl/diancan.xiaochengxu.phpblog.com.cn/diancan.xiaochengxu.phpblog.com.cn.key; root /home/www/diancan/xiaochengxu; # 该项要修改为你准备存放相关网页的路径 include /usr/local/nginx/ssl/ssl_headers; location / &#123; try_files $uri $uri/ /index.php?$query_string; index index.php index.html index.htm; &#125; location ~ \\.php$ &#123; include /usr/local/nginx/conf/fastcgi.conf; fastcgi_intercept_errors on; fastcgi_pass 127.0.0.1:9000; &#125; &#125; 123456789101112# out /usr/local/nginx/ssl/ssl_headersadd_header Strict-Transport-Security &quot;max-age=15768000; includeSubDomains; preload&quot; always;# out /usr/local/nginx/ssl/ssl_paramsssl_protocols TLSv1 TLSv1.1 TLSv1.2;ssl_dhparam /usr/local/nginx/ssl/dhparam.pem; # See https://weakdh.org/sysadmin.html for more detailsssl_session_cache shared:SSL:1m;ssl_session_timeout 5m;ssl_prefer_server_ciphers on;ssl_ciphers &quot;ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-DSS-AES128-GCM-SHA256:kEDH+AESGCM:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA:ECDHE-ECDSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-DSS-AES128-SHA256:DHE-RSA-AES256-SHA256:DHE-DSS-AES256-SHA:DHE-RSA-AES256-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES256-SHA:AES:CAMELLIA:DES-CBC3-SHA:!aNULL:!eNULL:!EXPORT:!DES:!RC4:!MD5:!PSK:!aECDH:!EDH-DSS-DES-CBC3-SHA:!EDH-RSA-DES-CBC3-SHA:!KRB5-DES-CBC3-SHA&quot;;dhparam.pem这个文件是我之前就生成好的，生成命令openssl dhparam -out /usr/local/nginx/ssl/dhparam.pem 2048 步骤五 重启nginx查看证书自动更新申请下来的证书有效期只有90天 123在crontab 中添加一条命令0 0 * * * /home/work/opbin/ssl/acme.sh-master/acme.sh --cron --home /home/work/opbin/ssl/acme.sh-master/acme.sh此处就是每天凌晨检查证书 证书会在60天的时候更新 因为acme会记住之前执行的installcert，所以更新完证书之后他会自动重启一下nginx 如果之前运行installcert的时候没有输入reloadcmd,则需要更新之后自己手动重启(这样就没有自动更新的意义了) 通过dns配置check权限手动配置步骤1123456789101112131415161718192021222324252627282930313233343536[work@iZ25ndyf9bxZ acme.sh-master]$ !1019./acme.sh --issue --dns -d *.test.com -d test.com --yes-I-know-dns-manual-mode-enough-go-ahead-please[Tue Sep 11 21:24:56 CST 2018] Creating domain key[Tue Sep 11 21:24:56 CST 2018] The domain key is here: /home/work/.acme.sh/*.test.com/*.test.com.key[Tue Sep 11 21:24:56 CST 2018] Multi domain=&apos;DNS:*.test.com,test.com&apos;[Tue Sep 11 21:24:56 CST 2018] Getting domain auth token for each domain[Tue Sep 11 21:24:59 CST 2018] Getting webroot for domain=&apos;*.test.com&apos;[Tue Sep 11 21:25:00 CST 2018] Getting webroot for domain=&apos;test.com&apos;[Tue Sep 11 21:25:00 CST 2018] Add the following TXT record:[Tue Sep 11 21:25:00 CST 2018] Domain: &apos;_acme-challenge.test.com&apos;[Tue Sep 11 21:25:00 CST 2018] TXT value: &apos;Oe0iBXj3QvUErZOpROldRLx5jpyXbazsX36lkI46C_Y&apos;[Tue Sep 11 21:25:00 CST 2018] Please be aware that you prepend _acme-challenge. before your domain[Tue Sep 11 21:25:00 CST 2018] so the resulting subdomain will be: _acme-challenge.test.com[Tue Sep 11 21:25:00 CST 2018] Add the following TXT record:[Tue Sep 11 21:25:00 CST 2018] Domain: &apos;_acme-challenge.test.com&apos;[Tue Sep 11 21:25:00 CST 2018] TXT value: &apos;qVFtVzCnBsj1omQcdU1m8180rUBO8V5AHDczFUHqsMY&apos;[Tue Sep 11 21:25:00 CST 2018] Please be aware that you prepend _acme-challenge. before your domain[Tue Sep 11 21:25:00 CST 2018] so the resulting subdomain will be: _acme-challenge.test.com[Tue Sep 11 21:25:00 CST 2018] Please add the TXT records to the domains, and re-run with --renew.[Tue Sep 11 21:25:00 CST 2018] Please check log file for more details: /home/work/.acme.sh/acme.sh.log[work@iZ25ndyf9bxZ acme.sh-master]$ ./acme.sh --renew --dns -d *.test.com -d test.com --yes-I-know-dns-manual-mode-enough-go-ahead-please[Tue Sep 11 21:31:18 CST 2018] Renew: &apos;*.test.com&apos;[Tue Sep 11 21:31:19 CST 2018] Multi domain=&apos;DNS:*.test.com,test.com&apos;[Tue Sep 11 21:31:19 CST 2018] Getting domain auth token for each domain[Tue Sep 11 21:31:19 CST 2018] Verifying:*.test.com[Tue Sep 11 21:31:24 CST 2018] Success[Tue Sep 11 21:31:24 CST 2018] Verifying:test.com[Tue Sep 11 21:31:27 CST 2018] Success[Tue Sep 11 21:31:27 CST 2018] Verify finished, start to sign.[Tue Sep 11 21:31:30 CST 2018] Cert success.这个上面说的是需要在dns中添加Domain: &apos;_acme-challenge.test.com&apos;TXT value: &apos;Oe0iBXj3QvUErZOpROldRLx5jpyXbazsX36lkI46C_Y&apos;与 Domain: &apos;_acme-challenge.test.com&apos;TXT value: &apos;qVFtVzCnBsj1omQcdU1m8180rUBO8V5AHDczFUHqsMY&apos; 生效后 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051[work@iZ25ndyf9bxZ acme.sh-master]$ ./acme.sh --renew --dns -d *.test.com -d test.com --yes-I-know-dns-manual-mode-enough-go-ahead-please[Tue Sep 11 21:31:18 CST 2018] Renew: &apos;*.test.com&apos;[Tue Sep 11 21:31:19 CST 2018] Multi domain=&apos;DNS:*.test.com,DNS:test.com&apos;[Tue Sep 11 21:31:19 CST 2018] Getting domain auth token for each domain[Tue Sep 11 21:31:19 CST 2018] Verifying:*.test.com[Tue Sep 11 21:31:24 CST 2018] Success[Tue Sep 11 21:31:24 CST 2018] Verifying:test.com[Tue Sep 11 21:31:27 CST 2018] Success[Tue Sep 11 21:31:27 CST 2018] Verify finished, start to sign.[Tue Sep 11 21:31:30 CST 2018] Cert success.-----BEGIN CERTIFICATE-----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-----END CERTIFICATE-----[Tue Sep 11 21:31:30 CST 2018] Your cert is in /home/work/.acme.sh/*.test.com/*.test.com.cer [Tue Sep 11 21:31:30 CST 2018] Your cert key is in /home/work/.acme.sh/*.test.com/*.test.com.key [Tue Sep 11 21:31:30 CST 2018] The intermediate CA cert is in /home/work/.acme.sh/*.test.com/ca.cer [Tue Sep 11 21:31:30 CST 2018] And the full chain certs is there: /home/work/.acme.sh/*.test.com/fullchain.cer [Tue Sep 11 21:31:30 CST 2018] It seems that you are using dns manual mode. please take care: The dns manual mode can not renew automatically, you must issue it again manually. You&apos;d better use the other modes instead.[Tue Sep 11 21:31:30 CST 2018] Call hook error. 生成成功后配置 123456789[work@iZ25ndyf9bxZ acme.sh-master]$ ./acme.sh --installcert -d *.xmanlegal.com \\&gt; --key-file /mnt/usr/ssl/xmanlegal.com/xmanlegal.com.key \\&gt; --fullchain-file /mnt/usr/ssl/xmanlegal.com/xmanlegal.com.key.cer \\&gt; --reloadcmd &quot;echo &quot;Asdf1234&quot; sudo -S /mnt/usr/sbin/nginx -s reload&quot;[Tue Sep 11 21:36:31 CST 2018] Installing key to:/mnt/usr/ssl/xmanlegal.com/xmanlegal.com.key[Tue Sep 11 21:36:31 CST 2018] Installing full chain to:/mnt/usr/ssl/xmanlegal.com/xmanlegal.com.key.cer[Tue Sep 11 21:36:31 CST 2018] Run reload cmd: echo Asdf1234 sudo -S /mnt/usr/sbin/nginx -s reloadAsdf1234 sudo -S /mnt/usr/sbin/nginx -s reload[Tue Sep 11 21:36:31 CST 2018] Reload success 末文证书级别测试相关技术博客","categories":[],"tags":[{"name":"nginx","slug":"nginx","permalink":"https://shiwenyuan.github.io/tags/nginx/"},{"name":"linux","slug":"linux","permalink":"https://shiwenyuan.github.io/tags/linux/"},{"name":"https","slug":"https","permalink":"https://shiwenyuan.github.io/tags/https/"}]},{"title":"开发自己的composer包","slug":"开发自己的composer包","date":"2019-08-06T03:38:17.000Z","updated":"2019-08-06T03:41:47.286Z","comments":true,"path":"posts/cjz2bazgy000eges6iuicqnrx.html","link":"","permalink":"https://shiwenyuan.github.io/posts/cjz2bazgy000eges6iuicqnrx.html","excerpt":"","text":"composer简介1Composer 是 PHP 的一个依赖管理工具。它允许你申明项目所依赖的代码库，它会在你的项目中为你安装他们。 准备工作 1.一个github账号 2.一个packagist账号 3.一台安装composer的开发机 packagist注册链接 github注册链接 composer注册链接 — # 发布流程 1.首先在github上创建一个项目2.把项目克隆到本地3.进入项目根目录初始化项目123456789101112131415161718192021222324252627282930313233343536373839➜ xdpframework git:(master) composer init Welcome to the Composer config generator This command will guide you through creating your composer.json config.Package name (&lt;vendor&gt;/&lt;name&gt;) [shiwenyuan/xdpframework]: Description []: a xdp frameworkAuthor [北行10000 &lt;13341007105@163.com&gt;, n to skip]: Minimum Stability []: devPackage Type (e.g. library, project, metapackage, composer-plugin) []: License []: MITDefine your dependencies.Would you like to define your dependencies (require) interactively [yes]? Search for a package: Would you like to define your dev dependencies (require-dev) interactively [yes]? Search for a package: &#123; &quot;name&quot;: &quot;shiwenyuan/xdpframework&quot;, &quot;description&quot;: &quot;a xdp framework&quot;, &quot;license&quot;: &quot;MIT&quot;, &quot;authors&quot;: [ &#123; &quot;name&quot;: &quot;北行10000&quot;, &quot;email&quot;: &quot;13341007105@163.com&quot; &#125; ], &quot;minimum-stability&quot;: &quot;dev&quot;, &quot;require&quot;: &#123;&#125;&#125;Do you confirm generation [yes]? Would you like the vendor directory added to your .gitignore [yes]? yes 此时目录下回程车一个composer.json文件，文件内容形如 12345678910111213&#123; &quot;name&quot;: &quot;shiwenyuan/xdpframework&quot;, &quot;description&quot;: &quot;a xdp framework&quot;, &quot;license&quot;: &quot;MIT&quot;, &quot;authors&quot;: [ &#123; &quot;name&quot;: &quot;北行10000&quot;, &quot;email&quot;: &quot;13341007105@163.com&quot; &#125; ], &quot;minimum-stability&quot;: &quot;dev&quot;, &quot;require&quot;: &#123;&#125;&#125; 4.编写自己的composer包4.1 此处我创建了一个src目录并实现了SayHello类123456➜ xdpframework git:(master) ✗ tree.├── README.md├── composer.json└── src └── SayHello.php 4.2 修改composer.json123456789101112131415161718&#123; &quot;name&quot;: &quot;shiwenyuan/xdpframework&quot;, &quot;description&quot;: &quot;a xdp framework&quot;, &quot;license&quot;: &quot;MIT&quot;, &quot;authors&quot;: [ &#123; &quot;name&quot;: &quot;北行10000&quot;, &quot;email&quot;: &quot;13341007105@163.com&quot; &#125; ], &quot;minimum-stability&quot;: &quot;dev&quot;, &quot;require&quot;: &#123;&#125;, &quot;autoload&quot;: &#123; &quot;psr-4&quot;: &#123; &quot;XdpFrameWork\\\\&quot;: &quot;src/&quot; &#125; &#125;&#125; 4.3 自动加载src目录123456789101112131415161718192021222324在4.2添加了src的加载运行composer install实现自动加载➜ xdpframework git:(master) ✗ composer installLoading composer repositories with package informationUpdating dependencies (including require-dev)Nothing to install or updateGenerating autoload files此时目录中会生成一个vendor 它实现了加载方法➜ xdpframework git:(master) ✗ tree.├── README.md├── composer.json├── src│ └── SayHello.php└── vendor ├── autoload.php └── composer ├── ClassLoader.php ├── LICENSE ├── autoload_classmap.php ├── autoload_namespaces.php ├── autoload_psr4.php ├── autoload_real.php ├── autoload_static.php └── installed.json 4.4 测试结果1234567891011121314创建test.php&lt;?php/** * Created by PhpStorm. * User: shiwenyuan * Date: 2018/8/2 13341007105@163.com * Time: 下午8:56 */require_once __DIR__.&quot;/vendor/autoload.php&quot;;use XdpFrameWork\\SayHello;SayHello::world();命令行中运行➜ xdpframework git:(master) ✗ php test.phphello world 4.5 忽略vendor、composer.json文件12修改.gitignore增加下面两行/vendor/ 4.6提交代码到 github 5. Packagist配置5.1 github关联到Packagist1234561.首先要在Packagist上登录2.点击顶部导航条中的Summit按钮3.在输入框中输入github上的仓库地址，如：https://github.com/shiwenyuan/xdpfarmwork4.然后点击Check按钮5.Packagist会去检测此仓库地址的代码是否符合Composer的Package包的要求6.检测正常的话，会出现Submit按钮，再点击一下Submit按钮，我们的包就提交到Packagist上了 5.2 配置自动同步1231.从Packagist点击个人中心点击profile获取token然后复制2.到github项目首页上点击settings、点击webhooks、点击addwebhook3.把https://packagist.org/api/bitbucket?username=USERNAME&amp;apiToken=TOKEN 复制到Payload URL，在下方输入密码后保存 5.3 打tag测试 引用包1composer require shiwenyuan/xdpframework 注意事项123如果下载不下来到话就换一下镜像composer config -g repo.packagist composer https://packagist.laravel-china.org刚发布上去可能会下载不到，没有同步过来-- 可以等一会在试试看","categories":[],"tags":[{"name":"php","slug":"php","permalink":"https://shiwenyuan.github.io/tags/php/"},{"name":"composer","slug":"composer","permalink":"https://shiwenyuan.github.io/tags/composer/"}]}]}